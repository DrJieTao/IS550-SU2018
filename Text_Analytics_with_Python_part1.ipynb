{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analytics with Python - Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text analytics, also known as Natural Language Processing (NLP), refer to the analytics toward text-based data.\n",
    "\n",
    "Text-based communication has become one of the most common forms of expression. We email, text message, tweet, and update our statuses on a daily basis. As a result, unstructured text data has become extremely common, and analyzing large quantities of text data is now a key way to understand what people are thinking.\n",
    "\n",
    "One of the biggest breakthroughs required for achieving any level of artificial intelligence is to have machines which can process text data. Thankfully, the amount of text data being generated in this universe has exploded exponentially in the last few years.\n",
    "\n",
    "It has become imperative for an organization to have a structure in place to mine actionable insights from the text being generated. From social media analytics to risk management and cybercrime protection, dealing with text data has never been more important.\n",
    "\n",
    "In this article we will discuss different feature extraction methods, starting with some basic techniques which will lead into advanced Natural Language Processing techniques. We will also learn about pre-processing of the text data in order to extract better features from clean data.\n",
    "\n",
    "By the end of this article, you will be able to perform text operations by yourself. Let’s get started!\n",
    "\n",
    "Original tutorial from [link](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/), revised and adapted by Dr. Tao.\n",
    "ver 1.0, May 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. Basic feature extraction using text data\n",
    "    - Load data\n",
    "    - Number of words\n",
    "    - Unique Words and Their Counts\n",
    "    - Other Counts of Words\n",
    "2. Basic Text Pre-processing of text data\n",
    "    - Lower case\n",
    "    - Removing Punctuation\n",
    "    - Removal of Stop Words\n",
    "    - Common words removal\n",
    "    - Rare words removal\n",
    "    - Spelling correction\n",
    "    - Tokenization\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "    - Part-of-Speach Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Text Feature Extraction\n",
    "\n",
    "We can use text data to extract a number of features even if we don’t have sufficient knowledge of Natural Language Processing. So let’s discuss some of them in this section.\n",
    "\n",
    "Before starting, let’s quickly read the training file from the dataset in order to perform different tasks on it. In the entire article, we will use the *twitter sentiment dataset* provided with NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class we are playing with Twitter data, NLTK has the data embedded - we can just load them from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is loaded as a JSON file - which can be intuitively loaded as a dict. Let's look at what are included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear the data is divided into three parts:\n",
    "    - positive;\n",
    "    - negative (these two are used for training in sentiment analysis);\n",
    "    - tweets.20150430... \n",
    "    \n",
    "In this particular exercise, we want to focus on the **positive** tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tweets = twitter_samples.strings('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at what is in our *pos_tweets* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(pos_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "Provide your code in the block below two list the first **10** tweets in the *pos_tweets* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Number of Words\n",
    "\n",
    "The first fact we want to know about text are word counts - let us start counting each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This will return the count for the first tweet\n",
    "print('The first tweet has %s words.' % str(len(pos_tweets[0].split(' '))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Provide the word counts for the first **10** tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### YOUR CODE HERE\n",
    "# use enumerate() here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this is not very data science like approach, so we are going to use a powerful tool provided in Python: **Pandas**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tweets = pd.DataFrame(pos_tweets, columns=['text'])\n",
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can count words in every tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tweets['word_count'] = pos_tweets['text'].apply(lambda x: len(str(x).split(\" \")))\n",
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the number of characters in each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tweets['char_count'] = pos_tweets.text.str.len()\n",
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also extract another feature which will calculate the average word length of each tweet. This can also potentially help us in improving our model.\n",
    "\n",
    "Here, we simply take the sum of the length of all the words and divide it by the total length of the tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use this function to calcualte the average word length in each tweet\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "pos_tweets['avg_word_length'] = pos_tweets.text.apply(lambda x: avg_word(x))\n",
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save this dataframe just in case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_tweets.to_csv('pos_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Unique Words and Their Counts\n",
    "\n",
    "You may have observed that the split() function is not well at identifying words - so we are going to use a built-in method with NLTK to identify words - namely tokenization. \n",
    "\n",
    "Refer to this [link](https://www.nltk.org/book/ch03.html) for more information.\n",
    "\n",
    "You should have noticed from the results below that we can capture the emojis from tweets using the TweetTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer() # This function is particularly used for tweets\n",
    "from collections import Counter\n",
    "vocab = Counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can process the count of unique words in the overall set of texts - namely **Corpus**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for text in pos_tweets.text:\n",
    "    n = tknzr.tokenize(text)\n",
    "    vocab.update(n)\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique token (words) in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('The corpus has %s unique words.' % str(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the top 10 unique words in the corpus, by their counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(vocab.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict(vocab.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does above list make any sense to you? Maybe not.\n",
    "\n",
    "Nonetheless, we should visualize the counter of unique words just because we can.\n",
    "\n",
    "We should just visualze the most 20 words because of the long running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "count_dict = dict(vocab.most_common(20))\n",
    "words = range(len(count_dict))\n",
    "plt.bar(words, count_dict.values(), color='blue', align='center', tick_label=count_dict.keys())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Other Counts of Words\n",
    "\n",
    "Sometimes we also care about other counts of words in a corpus, let's go through them.\n",
    "\n",
    "For instance, we noticed that the most common words from the counter (vocab) have less meanings - we call them **stopwords** (i.e. 'I', 'the', ...). Let's examine them by counts for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "pos_tweets['stop_counts'] = pos_tweets.text.apply(lambda x: len([x for x in x.split() if x in stop]))\n",
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also noticed that a great amount of special characters, such as *emojis*, appeared in the tweets. We can count them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = pos_tweets.text[0]\n",
    "print(tknzr.tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def count_special_char(sentence):\n",
    "    non_special = [tok for tok in tknzr.tokenize(sentence) if(re.search(r'^\\w', tok))]\n",
    "    return(len(non_special))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_special_char(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tweets['special_char'] = pos_tweets.text.apply(lambda x: len(tknzr.tokenize(x)) - count_special_char(x))\n",
    "pos_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also care about the number of numerics in each tweet.\n",
    "\n",
    "Just like we calculated the number of words, we can also calculate the number of numerics which are present in the tweets. It does not have a lot of use in our example, but this is still a useful feature that should be run while doing similar exercises. For example, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tweets['num_counts'] = pos_tweets.text.apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n",
    "pos_tweets[['text','num_counts']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Say we are interested in searching for all long words in the tweets. A long word is defined as:\n",
    "- has more than 5 letters;\n",
    "- has to be a word (not special character, numerics, ...)\n",
    "\n",
    "Please write you own code for the **count of long words** and add it as a column in *pos_tweets*.\n",
    "\n",
    "**HINT**: use code/results from above as much as you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "# define your function here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use apply(lambda) here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic Preprocessing\n",
    "\n",
    "So far, we have learned how to extract basic features from text data. Before diving into text and feature extraction, our first step should be cleaning the data in order to obtain better features. We will achieve this by doing some of the basic pre-processing steps on our training data.\n",
    "\n",
    "So, let’s get into it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Lower case\n",
    "\n",
    "The first pre-processing step which we will do is transform our tweets into lower case. This avoids having multiple copies of the same words. For example, while calculating the word count, ‘Analytics’ and ‘analytics’ will be taken as different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tweets['lower'] = pos_tweets.text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "pos_tweets.lower.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Removing Punctuation\n",
    "\n",
    "The next step is to remove punctuation, as it doesn’t add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_tok = RegexpTokenizer(r'\\w+')\n",
    "pos_tweets['no_punc'] = pos_tweets['lower'].apply(lambda x: ' '.join(reg_tok.tokenize(x)))\n",
    "pos_tweets.no_punc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the above output, all the punctuation, including ‘#’ and ‘@’, has been removed from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Removal of Stop Words\n",
    "\n",
    "As we discussed earlier, stop words (or commonly occurring words) should be removed from the text data. For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_tweets['no_stop'] = pos_tweets['no_punc'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "pos_tweets.no_stop.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.4 Common word removal\n",
    "\n",
    "Previously, we just removed commonly occurring words in a general sense. We can also remove commonly occurring words from our text data First, let’s check the 10 most frequently occurring words in our text data then take call to remove or retain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq = pd.Series(' '.join(pos_tweets['no_stop']).split()).value_counts()[:10]\n",
    "freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s remove these words as their presence will not of any use in classification of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freq = list(freq.index)\n",
    "pos_tweets['no_common'] = pos_tweets['no_stop'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "pos_tweets['no_common'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Rare words removal\n",
    "\n",
    "Similarly, just as we removed the most common words, this time let’s remove rarely occurring words from the text. Because they’re so rare, the association between them and other words is dominated by noise. You can replace rare words with a more general form and then this will have higher counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rare = pd.Series(' '.join(pos_tweets['no_common']).split()).value_counts()[-10:]\n",
    "rare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rare = list(rare.index)\n",
    "pos_tweets['no_rare'] = pos_tweets['no_common'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\n",
    "pos_tweets['no_common'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.6 Spelling correction\n",
    "\n",
    "We’ve all seen tweets with a plethora of spelling mistakes. Our timelines are often filled with hastly sent tweets that are barely legible at times.\n",
    "\n",
    "In that regard, spelling correction is a useful pre-processing step because this also will help us in reducing multiple copies of words. For example, “Analytics” and “analytcs” will be treated as different words even if they are used in the same sense.\n",
    "\n",
    "To achieve this we will use the textblob library. If you are not familiar with it, you can check my previous article on ‘NLP for beginners using textblob’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "pos_tweets['no_common'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the words are corrected accurately ('accnt -> accent'); also some are not ('rqst' -> 'rest')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Tokenization - Again\n",
    "\n",
    "Tokenization refers to dividing the text into a sequence of words or sentences. In our example, we have used the textblob library to first transform our tweets into a blob and then converted them into a series of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "twtkr = TweetTokenizer()\n",
    "twtkr.tokenize(pos_tweets['text'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOUR TURN HERE\n",
    "\n",
    "Select the last 10 tweets in the dataset (pos_tweets), then use following tokenizers to tokenize each tweet and output the results.\n",
    "- Regxp Tokenizer (reg_tok)\n",
    "- Tweet Tokenizer (twtkr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We already defined both tokenizer, now we just need to call them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Stemming\n",
    "\n",
    "Stemming refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. For this purpose, we will use PorterStemmer from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "st = SnowballStemmer(\"english\")\n",
    "pos_tweets['text'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter data is not very good from stemming because of the spelling and non plain English tokens (handles, tags, ...). However, to further prove that the stemming is working, try code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(st.stem('running'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are using the native Snowball stemmer; we can also use other stemmer within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Use Snowball stemmer on generously is: %s.' % str(SnowballStemmer(\"english\").stem(\"generously\")))\n",
    "print('Use Porter stemmer on generously is: %s.' % str(SnowballStemmer(\"porter\").stem(\"generously\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Lemmatization\n",
    "\n",
    "Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. It makes use of the vocabulary and does a morphological analysis to obtain the root word. **Therefore, we usually prefer using lemmatization over stemming.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "pos_tweets['text'][:5].apply(lambda x: \" \".join([wordnet_lemmatizer.lemmatize(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, tweets may not be the best for demonstrating *lemmatization*, so we check following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_sent = 'WordNet superficially resembles a thesaurus, in that it groups words together based on their meanings. \\\n",
    "However, there are some important distinctions.'\n",
    "[wordnet_lemmatizer.lemmatize(w) for w in reg_tok.tokenize(my_sent)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also define the Part-of-Speech in lemmatization. See code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordnet_lemmatizer.lemmatize('are', pos='v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2.10 Part-of-Speech Tagging\n",
    "\n",
    "Part-of-speech tagging is one of the most important text analysis tasks used to classify words into their part-of-speech and label them according the tagset which is a collection of tags used for the pos tagging. Part-of-speech tagging also known as word classes or lexical categories. Detailed definition can be found on [Wikipedia](https://en.wikipedia.org/wiki/Part-of-speech_tagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = 'Due to the nature of this tagger, it works best when trained over sentence delimited input.'\n",
    "from nltk import pos_tag\n",
    "pos_tag([w for w in reg_tok.tokenize(s)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that pos_tag by default takes **a list of strings** as input; if you input a string, pos_tag will split it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.\n",
    "We are going to load the book *Monty Python and Holy Grail* from the embedded text in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between following two lines? Which one will give a larger value of length? Will this be the case for other texts?\n",
    "\n",
    "    sorted(set(w.lower() for w in text1))\n",
    "    sorted(w.lower() for w in set(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.\n",
    "Define sent to be the list of words \n",
    "\n",
    "    ['she', 'sells', 'sea', 'shells', 'by', 'the', 'sea', 'shore']. \n",
    "\n",
    "Now write code to perform the following tasks:\n",
    "- Print all words beginning with **se**\n",
    "- Print all words longer than **four** characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Tutorial\n",
    "\n",
    "In this tutorial, we complete **basic feature extraction** and **basic text pre-processing** in this tutorial. We will continue with **advanced text processing** in Part 2 of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
